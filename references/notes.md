## Notes for Project  
---
## Articles
<details><summary>Blog Post: NLP, LDA, and Kmeans for Music Classification (in R)</summary>
<p>

[Link](https://www.datacamp.com/community/tutorials/ML-NLP-lyric-analysis)
### LDA Approach
#### Overview of LDA
This paper uses LDA and K-means to generate collections of words from documents that suggest themes.  
The first thing they did was clean the data, remove stop words, create tidy versions.    

Latent Dirichlet Allocation, LDA, is a way to try and discover the latent topic(s) which underlies a specific document. LDA assumes that every document is a combination of one or more of these latent topics. It's like LDA algorithmically goes through words and finds groups of words and makes "clusters" from them which we can give names to as our "topics".   

The idea underpinning this algorithm is that the words which make up one of these topics will appear together in documents. Thus each document gets modeled as a mixture of topics and these topics are themselves defined as a mixture of some words. Then based on the makeup of the words in a document, you can assign a probability that it comes from one of these latent topics.  

#### Algorithm  
1. During initialization, each word is assigned to a random topic  
2. The algorithm goes through each word iteratively and re-assigns the word to a topic with the following considerations:  
* the probability the word belongs to a topic  
* he probability the document will be generated by a topic    
  
#### Applying it to data  
First thing is to create a document-term-matrix (DTM) in which every piece of vocabulary in the corpus are the columns, and every row is a specific document. Thus each value in the DTM is how many times that word is used in that document.    

The key parameters to the algorithm is the k, number of latent topics to assume.    
  
Then the output gives you an associated probability for every single word for every of your k topics. So for the word 'iceberg', you have an assigned score for each of the 3 topics.   

Then you go through your topics and find the words with the highest scores for those topics, and you can see how the algorithm is sort of 'defining' these topics.   

Now we have our words with their topic scores, and so we have our topics 'defined'. Now we can go through our documents, and give the documents scores for each topic! Just as words have a score for every topic, so do documents get a score for every topic.   

We could now look at the topics and which documents fall most heavily into these topics. 

### K-means Approach
#### Overview
K-means isn't going to give a topic score like LDA to each document, it's going to be an all or nothing classification. It is first going to transform each document to a numeric vector and then cluster on 'distance' between them.

### Takeaways  
This is in R so it's not super applicable but it does have good details of LDA and how you could use it and how to think about classifying these documents with LDA and possibly Kmeans. 

</p>
</details>

<details><summary>Blog Post: Word2Vec</summary>
<p>

[Link](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model)

#### The Model
We will be talking about a specific implementation of Word2Vec, called the skip-gram neural network model. This strategy uses a neural network with one hidden layer, and the weights of these hidden layers end up being the 'word vectors' whcih we are trying to learn. This may sound familiar to an auto-encoder. 

#### The Fake Task
We have to set our neural network up to perform a fake task, which later we will come back to and find that by telling our NN to do this, it generated our vectors!
**The Task**: Given a specific word a certain location in a sentence, predict what words will be nearby. This is like a multi-class big classification problem where you want to get probabilities for every other word in your vocabulary to be 'near' this chosen input word. The output probabilities are going to relate to how likely it is find each vocabulary word nearby our input word. For example, if you gave the trained network the input word “Soviet”, the output probabilities are going to be much higher for words like “Union” and “Russia” than for unrelated words like “watermelon” and “kangaroo”.

#### Model Details
So... we're going to need some way to put these words into the neural network. We first build our 'vocabulary' from all of our documents, where we have all of our columns being each of these words, then every word will be represented as a "one-hot" vector, aka a vector where there is a `1` in the column with this word and a `0` elsewhere.

#### The Hidden Layer
Remember the hidden layer is all the different ways to combine your input features, and then those get combined together in order for you to get your output probabilities. For this example, we want each word to represented by a 300 vector representation. Thus we will have 300 nodes in our hidden layer, so that for each word we get an associated weight for each of these 300 nodes. If we think of all of this information as a matrix, where each row is a unique word in our vocabulary, and each column is one of these 300 new feature, each of these rows is now the associated vector for these initial words! We have created vectors from our words, aka 'WordVectors'.

#### TakeAway
If two different words have very similar "contexts", that is they will have similar words which appear around them at high probabilities, then the output from this model for an input of either of these words should be similar. Our network will create the weights to be similar so that these two words have similar word vectors. 

</p>
</details>

<details><summary>Blog Post: Doc2Vec</summary>
<p>

[Link](https://medium.com/scaleabout/a-gentle-introduction-to-doc2vec-db3e8c0cce5e)

### Overview
#### Intro
BOW is one way to represent documents with numbers, but it is pretty crude. LDA is another method. word2vec and doc2vec are alternatives to these, but I will possibly be using them in conjunction with one another? 

#### Word2Vec
word2vec is a concept which is used to map words on to some n-dimensional feature universe. It's a way to make it so that words which appear in similar contexts have a similar vector representation, not just that every word is arbitrarily related to other words. See the above summary of Word2Vec for more details. Ideally, analogies will hold in this word2vec feature space, like man:king should be the same distance as woman:queen are from each other. 

#### Doc2Vec
So the goal of Doc2Vec is create a numeric representation in an n-dimensional feature space, of a document regardless of its length. This is pretty similar to word2vec. Remember for word2vec, the input vector was a one-hot encoding of the word; well now we have a similar encoding where the input is at the document level, and it has a `1` where a word that is included is present, an additional marker for what document this is. This creates document vectors instead of word vectors. 

#### How to use
For training, a set of documents is required. A word vector is generated for each word, and a document vector is generated for each document. In the inference stage, a new document may be presented, and the previously found weights are used to calculate the document vector. 

</p>
</details>

<details><summary>Github Repo, Book Genre From Titles</summary>
<p>

[Link](https://github.com/akshaybhatia10/Book-Genre-Classification)

### README.md
#### Overview
This project classifies book sinto genres based only on titles. There are 32 genres to classify the books in it. There are two notebooks relevant to me: Basic_Bag_of_Words_model.ipynb & Best_TFIDF-Vectorizer_model.ipynb.  

</p>
</details>

<details><summary>Github Repositories, Music Genre Classification</summary>
<p>

[1st Link](https://github.com/dipayandutta93/Music-Genre-Classification-using-lyrics)  
[2nd Link](https://github.com/ianscottknight/Musical-Genre-Classification-of-Song-Lyrics)  

### Link 1
#### Overview

</p>
</details>

<details><summary>Blog Post: Text Classification in Python</summary>
<p>

[Link](https://www.analyticsvidhya.com/blog/2018/04/a-comprehensive-guide-to-understand-and-implement-text-classification-in-python/)

### Setup 
First the article recommends some packages to import: Pandas, sklearn, XGBoost, TextBlob, Keras. The corpus in this case is a couple million labeled amazon reviews as documents.   

### Feature Engineering
They then do some feature engineering, creating input features for our supervised learning model we will build. This is different than taking an unsupervised approach. 

**Count Vectors as a Feature**  
A count vector is creating the DTM discussed in the 'NLP, LDA, and Kmeans for Music Classification' linked above. It has rows as documents and columns as the vocabulary of the corpus.

**TF-IDF Vectors as a Features**  
TF-IDF gives a score to a word. The TF part is the term frequency, so for a word in a document, how often does that word appear in the document? You can normalize by the size of the document, so like what % of the words in the document does this specific word make up?  

Then the IDF part is inverse document frequency, so how few documents actually contain this term. So the TF-IDF is TFxIDF. The point of TF-IDF is to give like a 'uniqueness' score to each word for a specific document, like how much does this word separate this document into some special document category. 

Let's run through a couple examples to get it better. Let's go through 4 scenarios:
* High TF, High IDF = In this document, this term shows up a lot and makes up a high percentage of this document. Also we will multiply this because not many documents have this word in it. Thus this word is important at differentiating this document into its category.
* High TF, Low IDF = In this document this term shows up a lot, but it is also a term which shows up a lot in other documents, so let's lower the weight. 
* Low TF, High IDF = This term doesn't show up that much in this document, but it is a very rare word.
* Low TF, Low IDF = This term doesn't show up very often and it's not a very rare word in the corpus. 

**Word Embeddings**
A word embedding is a form of representing words in a vector space, like in an actual 3D or 4D or n-dimensinoal space. The position of a word within this n-dimensional space is learned from the text, and based on what kinds of words surround it and are normally used with it. You can use pre-trained word embeddings, like word2vec. 

**LDA/Topic Modeling**

### Building the Model
With all these features above, you can build a model with them as the inputs. 

</p>
</details>

<details><summary>Blog Post: NLP basics w/ Python (not only text class.)</summary>
<p>

[Link](https://www.analyticsvidhya.com/blog/2017/01/ultimate-guide-to-understand-implement-natural-language-processing-codes-in-python/)

</p>
</details>

<details><summary>Blog Post: Word Embeddings</summary>
<p>

[Link](https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/)

</p>
</details>  

<details><summary>LDA w/ sklearn</summary>
<p>

[Link](https://medium.com/mlreview/topic-modeling-with-scikit-learn-e80d33668730
)

#### LDA
```python
# get our documents into BOW style.
tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=1000, stop_words='english')
tf = tf_vectorizer.fit_transform(documents)
tf_feature_names = tf_vectorizer.get_feature_names()

# Run LDA
from sklearn.decomposition import LatentDirichletAllocation
lda = LatentDirichletAllocation(n_topics=8, max_iter=5, learning_method='online', learning_offset=50.,random_state=0).fit(tf)

# display
def display_topics(model, feature_names, no_top_words):
    for topic_idx, topic in enumerate(model.components_):
        print "Topic %d:" % (topic_idx)
        print " ".join([feature_names[i]
                        for i in topic.argsort()[:-no_top_words - 1:-1]])

display_topics(lda, tf_feature_names, 10)

```
</p>
</details>  

<details><summary>Custom sklearn Transformers for pipelines</summary>
<p>

[Link](https://towardsdatascience.com/custom-transformers-and-ml-data-pipelines-with-python-20ea2a7adb65)

#### Overview
Remember, Sklearn pipelines are composed of steps, all of which are transforms until the final model fitting. To implement this we take advantage of class inheritance in Python, because we know from Sklearn syntax that we do a lot of class instantiation and stuff in python. The following is a helpful example of normal Sklearn syntax
```python
from sklearn.preprocessing import OneHotEncoder 

#Initializing an object of class OneHotEncoder
one_hot_enc = OneHotEncoder( sparse = True )

#Calling methods on our OneHotEncoder object
one_hot_enc.fit( some_data ) #returns nothing
transformed_data = one_hot_enc.transform( som_data ) #returns something
```
We see above that we create an instance of the OneHotEncoder transformer with some settings, and then we can call the method `.transform` on from that object instance, and give it some_data as an argument. 

Thus when we create our own, we need it to be a class with methods such as `.fit` and `.transform` etc. to fit in with the other transformers. 

#### Getting Started
Sklearn gives us two goo base classes with which to inherit from in order to write out own transformers: `TransformerMixin` & `BaseEstimator`. Inheriting from TransformerMixin ensures that all we need to do is write our fit and transform methods and we get fit_transform for free. Inheriting from BaseEstimator ensures we get get_params and set_params for free. Since the fit method doesn’t need to do anything but return the object itself, all we really need to do after inheriting from these classes, is define the transform method for our custom transformer and we get a fully functional custom transformer that can be  integrated with a scikit-learn pipeline!

#### Example

```python
#Custom Transformer that extracts columns passed as argument to its constructor 
class FeatureSelector( BaseEstimator, TransformerMixin ):
    #Class Constructor 
    def __init__( self, feature_names ):
        self._feature_names = feature_names 
    
    #Return self nothing else to do here    
    def fit( self, X, y = None ):
        return self 
    
    #Method that describes what we need this transformer to do
    def transform( self, X, y = None ):
        return X[ self._feature_names ] 
```
As we can see above, we have created a custom transformer called FeatureSelector, which in order to use we will simply instantiate, and it will already have all of the things from BaseEstimator and TransformerMixin so that's nice. Then we have customized the `.transform()` function to return what we want it to return, based on the arguments which are given when it is called. We could even define other helped functions in this class and then call them in the transform function. Also notice that it must `return` what we want it to return, which is most of the time some minorly altered version of itself.


</p>
</details>  

do this one also! https://stackabuse.com/python-for-nlp-topic-modeling/
and this one! https://towardsdatascience.com/unsupervised-nlp-topic-models-as-a-supervised-learning-input-cf8ee9e5cf28 

<details><summary>Deploying a ML Model w/Flask</summary>
<p>

[Link to Blog](https://blog.cambridgespark.com/deploying-a-machine-learning-model-to-the-web-725688b851c7)

[Link to Code](https://github.com/alexanderrobertson/cambridgespark-webapp)

#### 1.) Creating Your Model
After instantiating a model object with parameters set, then `.fit`ting it to our data, we have a trained model object. We can save this trained model object using pickle:
```python
import pickle
with open('bike_model_xgboost.pkl', 'wb') as file:
pickle.dump(classifier, file)
```

#### 2.) Creating My Web App to Deploy This Model
There are 2 main components to this Flask App:
* The Python code that loads our model --> gets user input from web form --> makes predictions --> serves results
* The HTML templates that render with Flask

A simple visual skeleton for this is
```
webapp/
    ├── model/
    │   └── bike_model_xgboost.pkl
    ├── templates/
    │   └── main.html
    └── app.py
```

**app.py**
* `app.py` is the core of the web app; it is what will run on our EC2 server, send out web pages, and process input from users. 
* In flask, URLs get routed to different functions.
* Going to a url will trigger the function associated with that route. Likely one of the functions triggered will be a `flask.render` which will serve an `.html` webpage you create.
* Let's disect the below example
```python
import flask
app = flask.Flask(__name__, template_folder='templates')
@app.route('/')
def main():
    return(flask.render_template('main.html'))
if __name__ == '__main__':
    app.run()
```
When this script is run, `app.run()` will be the first thing executed. This will call the run method on the app variable, which is a flask object we created in the beginning of the file. Now our app is running, and when someone navigates to the route `'/'` they will get the rendered `main.html` file.

**main.html**
An .html file we can set up. Something simple like below will work:
```html
<!doctype html>
<html>
<head>
<title>Web app name</title>
</head>
<h1>Hello world.</h1>
</html>
```

**running this app.py**
I think we don't actually have to call `python app.py` we can just call `flask run` from command line and it will find that file and run it. This will launch out app locally. Running the function associated with the `/` route which will serving up our .html file to a local port for us to view in browser. 

**Modifying to Take User Input, Run Model, and Spit Back Prediction**
It is a few simple steps to achieve this goal.
* Edit app.py file to load the model
* Expand `main()` (which was our function called when the `/` route was navigated to) to prompt for user input, retrieve user input, make predictions and return predictions.
* Edit main.html to have a form that gets the input we need, allows users to submit input and renders predictions.

**Edit app.py to Load Model**
We can load in this pickled model into our app.py file. Doing this at the top of the app means it will only get loaded once, not every time the `/` route is called. A little firther down we will see this new app.py in full.

**Expand main() in app.py**
Our app will run in 2 modes, one of which displays the input form to the user, one of which retreives that input. The equivalent and necessary HTTP methods to use are `GET` & `POST`. 

As soon as a user navigates to the `/` route, flask receives a `GET` request, and the main function is called rendering our `main.html`. We will set things up so that if the user fills out the input form and clicks `submit`, flask will receive a `POST` request, and we will have our app extract the input, run it through whatever data cleaning is necessary, get a prediction from our model object, and render `main.html` with the results in place!

Flask extracts the data received from the `POST` request with `flask.request.form`, which we will see in action later. We will have to set up our `main.html` to allow an input form and data to be saved, which we wil grab with the aforementioned flask.request.form. 

Once we've grabbed our variables from parsing our user input out of the main.html, we can get the data into input form, and run it through our model pipeline. We then can get the model results.

Finally, we can re-render our main.html with the input from the user, and our model results/predictions. 

```python
import flask
import pickle
# Use pickle to load in the pre-trained model.
with open(f'model/bike_model_xgboost.pkl', 'rb') as f:
    model = pickle.load(f)
app = flask.Flask(__name__, template_folder='templates')

@app.route('/', methods=['GET', 'POST'])
def main():
    if flask.request.method == 'GET':
        return(flask.render_template('main.html'))
    if flask.request.method == 'POST':
        temperature = flask.request.form['temperature']
        humidity = flask.request.form['humidity']
        windspeed = flask.request.form['windspeed']
        input_variables = pd.DataFrame([[temperature, humidity, windspeed]],
                                       columns=['temperature', 'humidity', 'windspeed'],
                                       dtype=float)
        prediction = model.predict(input_variables)[0]
        return flask.render_template('main.html',
                                     original_input={'Temperature':temperature,
                                                     'Humidity':humidity,
                                                     'Windspeed':windspeed},
                                     result=prediction,
                                     )

if __name__ == '__main__':
    app.run()
```
Let's look a little closer at this. Of course when this is run, obviously app.run() will run, and it will `run` our app object, a flask instance. Our model has been loaded in under the variable name `model`. We have 2 possible methods, GET & POST, and depending on which method is given to the app, different functionality is enacted. 

If `GET` is received by the flask app, it simply renders our main.html. 
If `POST` is received, it first get's the variable names from the user input. Then it creates a DF with those variables, and after some cleaning gets a prediction from them with our model object. Then we return a `flask.render` which will serve that prediction up to the user. We will see specific below on how the .html was modified to allow this functionality.

**Before We do That, a Brief Aside on HTTP & HTTP Request Methods**
HTTP (hypertext transfer protocol) is a way for servers and clients to communicate. It is a request-response protocall to mediate communication. Normally some computer with an application running a website is the host server, and some other person's computer with a web browser is the client. The browser client submits an http request to the host server site, and the server returns a response to that client. The response hopefully contains the requested content.

A client browser can send different types of requests to this host server, and based on the type of request the host server can decide what to send back. 

`GET` Request: 
When a client sends a GET request, they are asking the host server for the host to send back some data. The query string (name/value pairs) is sent in the URL of a GET request.

`POST` Request:
When a client sends a POST call, they are sending some data to the server, for the host server to use however it pleases, likely to create or update some resource. The data sent to the server with POST is stored in the request body of the HTTP request.

**Back to the Main Event: Editing the main.html template**
We need to expand the template to include a user input form, as well as a way of determining if results need to be rendered, and if they do, then some code to render them appropriately. 

Within our main.html we will include a form section, something like:
```html
<form action="{{ url_for('main') }}" method="POST">
```
The action attribute tells flask which route, and therefore function, should be called when this form is submitted by the user in a valid way. The POST method tells the function that it should expect to be receiving input and that we are expecting input and that we will need a way to grab and process this input. We will likely include a `required` attribute because this input is required from the user.

We will also include a `div` container to display our results. Of course this container should only be displayed if we have results to display, so it will need to be in some way conditional. Within this container we will include some fields that may look off, they have `{}` curly braces, and these are not normal html, but flask knows how to work with them. Within these we can put placeholder variables which we will be able to pass arguments to from out app.py functions when we make the render call to this specific template! Things are starting to come together...

Anyways, we will also add some CSS for appearance. And below we will show the new and improve html file and run through it.

```html
<!doctype html>
<html>
<style>
form {
    margin: auto;
    width: 35%;
}
.result {
    margin: auto;
    width: 35%;
    border: 1px solid #ccc;
}
</style>
<head>
    <title>Bike Usage Model</title>
</head>
<form action="{{ url_for('main') }}" method="POST">
    <fieldset>
        <legend>Input values:</legend>
        Temperature:
        <input name="temperature" type="number" required>
        <br>
        <br> Humidity:
        <input name="humidity" type="number" required>
        <br>
        <br> Windspeed:
        <input name="windspeed" type="number" required>
        <br>
        <br>
        <input type="submit">
    </fieldset>
</form>
<br>
<div class="result" align="center">
    {% if result %}
        {% for variable, value in original_input.items() %}
            <b>{{ variable }}</b> : {{ value }}
        {% endfor %}
        <br>
        <br> Predicted number of bikes in use:
           <p style="font-size:50px">{{ result }}</p>
    {% endif %}
</div>
</html>
```
Actually before we get to the grand finale where we dive into this html, we are going to watch the first 2-3 corey schafer videos on flask apps to really set us up for success and make this worthwhile.

**Video 1: Getting Started**
[link to video 1](https://www.youtube.com/watch?v=MwZwr5Tvyxo)
* Install `flask` package
* Your `.py` file will be the basis of your application. 
* We create multiple pages using route and route decorators. 
* To run your app
    - One Method
        + Navigate to wherever your `.py` file is. 
        + There are a couple methods to actually run the app, he recommends first setting an environment variable to this `.py` file so the comp knows where to look.
        + `export FLASK_APP=app.py`
        + Then you can just use `flask run` from this directory and it will find that `app.py` and run it.
        + This will actually instantiate a running web host server which you can browse from your client, aka your browser. See the little section above on `HTTP` for a little more detail.
        + You can just paste the ip address it gives you in your client (browser) to access this running web server. You could also put in `loalhost:5000` or whatever port number it gives you and that will take you to the same thing.
    - Alternative Method
        + At the bottom of your `app.py`, include the below section
        + From command line, then simply call the script with python
        `python app.py`
``` python
if __name__=='__main__':
    app.run(debug=True)
```

* It's a bit of a pain to have to stop the server and restart it to see updated changes, so we can run it in debug mode which allows us to avoid this
    - `export FLASK_DEBUG=1`

* Creating new routes
    - In your `app.py`, you can create new routes by doing something similar to below
``` py
@app.route("/")
@app.route("/home")
def home():
    return 'home page! you landed here'

@app.route("/about")
def about():
    return 'about page! you landed here'
```
This above example shows us that we have 2 routes which will land on you on a the same rendered home page served to you by the host server and one /about route which asks the host server to get the about page and serve it up to the client browser.

**Video 2: Templates & Passing Variables To Webpage**
[link to video 2](https://www.youtube.com/watch?v=QnDWIZuWYW0)
* It's possible to return html directly in the route function within a multi-line string, but we can see how messy that would get. So we use templates.
* Within a templates directory we will create templates for our pages. There is a way to just make one template and re-use it and that will come later.
* Then we will just tell our route function to render this template. An example of this syntax is above so I won't give one here.
* Now what if we want to pass information into this template, possibly python variables, and then render those variables, let's find out how to do that!
    - Within the `render_template` function, simply set some variable equal to some local python variable, and we will now have access to that variable by whatever we set the parameter name as. e.g.:
```python
@app.route('/home')
def home():
    return render_template('main.html', posts=post_variable)
```
* Now within the `main.html`, we could actually access this variable `posts` with some special syntax we will learn now.
    - A code block is represented within our .html by curly braces and percent signs, and we also have to signify when it ends with another
```html
<body>
    {% for post in posts%}
        <h1>{{ post['title'] }}</h1>
        <p>Written by:{{ post['author'] }}</p>
    {% endfor %}
</body>
```

OK so that was an example of how to do a foor loop to generate html, we can also do some if/else control flow for out html template.

```html
<head>
    {% if title %}
        <title> User Submitted Title - {{ Post['title'] }} </title>
    {% else %}
        <title> Default Title </title>
    {% endif %}
</head>
```
* OK so now we have seen some neat stuff. One more thing to think about is the fact that we now have 2 templates, one for home page and one for out about page, and each of them have their own `.html` file to render from within their route function. But we want some things to be kept in common and styles to be similar, etc... We can accomplish this by using template inheritance, having one main template, and templates for the individual pages override just certain sections of the original template.

Below is a child template, inheriting from a `layout.html` parent template, and then overriding the place in the parent template specified as the 'content' block.
```html
{% extends "layout.html" %}

{% block content %}
    {% if title %}
        <title> User Submitted Title - {{ Post['title'] }} </title>
    {% else %}
        <title> Default Title </title>
    {% endif %}
{% endblock content %} 
```
Keep in mind that this only works if our `layout.html` file has the 
`{% block content %}{% endblock content %}` block somewhere in it.

* Well now with templates and variables and such we have some good stuff cookin', but we prob want to beautify things a bit. One good way to do that is using `bootstrap` for adding styles. 
    - They have a recommended template to use which loads in bootstrap and stuff.
    - We can use the flask.bootstrap extension if that seems simpler but otherwise we can just do this 

If we have set this up correctly we should not be able to assign our html elements to classes and have the css and stuff preconfigured by bootstrap. Lookup bootstrap classes for more info. If we do something like this:
```html
<div class="container">
    {% block content %}{% endblock %}
</div>
```
then now the stuff we populate this content block with and thus our div will be stylized with this bootstrap style.

Corey has some code snippets for a navigation bar and some global styles, so we can copy those from the description in this video I believe. We can put these code snippets of html into the parent template so the objects are present in all of our pages. 

* One more thing, we will want to create a directory called `static/` which houses our html and css files.  
* We are also going to want to `from flask import url_for`. Then go start at 28:30 of this video to see where he uses this url_for in the html tepmlate in order to tell it to look for its css in the correct location.  

At this point we have a solid starting basis, get to it and stop reading tutorials to avoid doing some mucky work!


</p>
</details>  
  
## Things to google search  
* `"genre" classification "book" machine learning text "NLP”`  
* `supervised document classification python`  
* `neural network sklearn multilabel` 

## Misc Notes
<details><summary>Sklearn</summary>
<p>

* **Basic Sklearn syntax**:
```python

[1]: from sklearn.linear_model import Lasso
[2]: from sklearn.model_selection import train_test_split

[3]: X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.4, random_state=42)
[4]: lassoRegObj = Lasso(alpha=0.4)
[5]: lassoRegObj.fit(X_train, y_train)
[6]: lassoRegObj.predict(X_test)
```
The first thing I do here is import my model which I want to use and some more stuff which will be useful, the ability to split my data into a training and testing set. Then I actually perform this split in line 3.

Starting in line 4, I actually begin constructing and fitting my model. Sklearn takes full advantage of python classes and object oriented programming. When you want to fit a model, you first instantiate a model object of that type. You can think of this as retreiving an out-of-the-box fresh model object and then customizing it with your parameters. In line 4 we see this occur, as I save to the `lassoRegObj` an instantiation of a lasso regression object with its alpha value tuned to `0.4`. 

This object has a method, `.fit()` which will customize this model object even more, this time molding it to fit the data which we provide, which is provided as the variables X_train and y_train in this example. 

Now that we have molded this model object to fit our training data, we can have it predict some new data which we provide. 

It is important to realize that you don't have to resave the object every time, because the .fit() is a method which changes the internal state of the model object. 

* **Sklearn CV syntax**: 
```python
[1]: from sklearn.model_selection import GridSearchCV

[2]: param_grid = {'n_neighbors':np.arange(1,50)}
[3]: knn = KNeighborsClassifier()
[4]: knn_cv = GridSearchCV(knn, param_grid, cv=5)
[5]: knn_cv.fit(X,y)
[6]: knn_cv.best_params_
```
OK here we are implementing CV with a model in sklearn. We set up the grid of parameters we would like to search by creating a dictionary called param_grid, where the key is actually the name of one of the hyperparameters which need tuning. 

Then we instantiate our KNN object called `knn`, which is just an out-of-the-box classifier with no customization. But we don't just want to fit this model, we want to fit this model many times each with different hyperparameters. So we need to instantiate a different model with this functionality. That is why on line 4, we instantiate a GridSearchCV object. This object is given our knn model, the hyperparameter ranges to test, and the # of cv folds as its parameters.

Now, as we have many times in the past, we mold this model to fit our data with the `knn_cv.fit()` method. It will fit our chosen model a bunch of times and give us back the best one.  

* **Sklearn Pipeline syntax**: 
```python
[1]: from sklearn.preprocessing import StandardScaler

[2]: X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.4, random_state=42)

[3]: steps = [('scaler', StandardScaler()),
              ('knn', KNeighborsClassifier())]
[4]: pipeline = Pipeline(steps)

[5]: knn_scaled = pipeline.fit(X_train, y_train)
[6]: y_pred = pipeline.predict(X_test)
```
So here we see an example of combining a pre-proccessing step which we call a 'transformation' into a Pipeline. The first thing we do is import some things, then we split up our data set. 

We next construct the steps of our pipeline. In this case, the pipeline has 2 steps, a transformation step called scaler which uses a StandardScaler() object, and then a model fitting step called 'knn' which uses an off-the-shelf KNN. 

Now, we actually instantiate a pipeline object, and this object is the thing which we will now mold and fit to our data. We see, we call `pipeline.fit()` similar to how in the past we have called our model objects, or cv_model objects `.fit()`.

* **Sklearn CV w/Pipeline syntax**: 
```python
[1]: steps = [('scaler', StandardScaler()),
         ('knn', KNeighborsClassifier())]

[2]: pipeline = Pipeline(steps)

[3]: CV_search_parameters = {knn__n_neighbors=np.arange(1, 50)}

[4]: X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=12345)

[5]: cv = GridSearchCV(pipeline, param_grid=CV_search_parameters)

[6]: cv.fit(X_train, y_train)

[7]: y_pred = cv.predict(X_test)
```
Now we are combining a lot of the things we have seen thus far to use CV and also take advantage of pipelines. We begin by creating the steps of our pipeline. This is similar to having a model, but instead of just being a model, it can be a couple transformation steps prior to the model. That is the point of the pipeline, to package it all up into one thing.

As earlier, in line 3, we see us setting which hyperparameters we want to tune through, but because now our Pipeline object can actually contain multiple objects itself, we must identify which pipeline step and what parameter in that step we want to include in out grid search. 

OK so in line 5 we actually instantiate this GridSearchCV object as we did before and we are going to fit it to our data, and instead of just giving it a model object like KNN, we give it an entire pipeline which is needs to run on all the CV folds. 

</p>
</details>  
