## Notes for Project  
---
### Articles
<details><summary>NLP, LDA, and Kmeans for Music Classification (in R)</summary>
<p>

[Link](https://www.datacamp.com/community/tutorials/ML-NLP-lyric-analysis)
## LDA Approach
### Overview of LDA
This paper uses LDA and K-means to generate collections of words from documents that suggest themes.  
The first thing they did was clean the data, remove stop words, create tidy versions.    

Latent Dirichlet Allocation, LDA, is a way to try and discover the latent topic(s) which underlies a specific document. LDA assumes that every document is a combination of one or more of these latent topics. It's like LDA algorithmically goes through words and finds groups of words and makes "clusters" from them which we can give names to as our "topics".   

The idea underpinning this algorithm is that the words which make up one of these topics will appear together in documents. Thus each document gets modeled as a mixture of topics and these topics are themselves defined as a mixture of some words. Then based on the makeup of the words in a document, you can assign a probability that it comes from one of these latent topics.  

### Algorithm  
1. During initialization, each word is assigned to a random topic  
2. The algorithm goes through each word iteratively and re-assigns the word to a topic with the following considerations:  
* the probability the word belongs to a topic  
* he probability the document will be generated by a topic    
  
### Applying it to data  
First thing is to create a document-term-matrix (DTM) in which every piece of vocabulary in the corpus are the columns, and every row is a specific document. Thus each value in the DTM is how many times that word is used in that document.    

The key parameters to the algorithm is the k, number of latent topics to assume.    
  
Then the output gives you an associated probability for every single word for every of your k topics. So for the word 'iceberg', you have an assigned score for each of the 3 topics.   

Then you go through your topics and find the words with the highest scores for those topics, and you can see how the algorithm is sort of 'defining' these topics.   

Now we have our words with their topic scores, and so we have our topics 'defined'. Now we can go through our documents, and give the documents scores for each topic! Just as words have a score for every topic, so do documents get a score for every topic.   

We could now look at the topics and which documents fall most heavily into these topics. 

## K-means Approach

</p>

</details>

<details><summary>Title</summary>
<p>

[Link](https://medium.com/@hakanakyurek)

* hakanaska@gmail.com

</p>
</details>

</details>

<details><summary>Title2</summary>
<p>

[Link](https://github.com/akshaybhatia10/Book-Genre-Classification)

</p>
</details>

<details><summary>Title3</summary>
<p>

[Link](https://www.quora.com/What-are-some-really-interesting-machine-learning-projects-for-beginners)

</p>
</details>

<details><summary>Title4</summary>
<p>

[Link](https://www.analyticsvidhya.com/blog/2018/04/a-comprehensive-guide-to-understand-and-implement-text-classification-in-python/)

</p>
</details>

<details><summary>Title5</summary>
<p>

[Link](https://www.digitalvidya.com/blog/document-classification-python-machine-learning/)

</p>
</details>

<details><summary>Title6</summary>
<p>

[Link](https://www.kaggle.com/c/wise-2014/overview)

</p>
</details>

<details><summary>Title7</summary>
<p>

[Link](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html#sklearn.neural_network.MLPClassifier)

</p>
</details>  

<details><summary>Title8</summary>
<p>

[Link](http://ethen8181.github.io/machine-learning/clustering_old/topic_model/LDA.html#content)

</p>
</details>  

  
### Things to google search  
* `"genre" classification "book" machine learning text "NLP‚Äù`  
* `supervised document classification python`  
* `neural network sklearn multilabel` 
