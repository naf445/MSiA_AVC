## Notes for Project  
---
## Articles
<details><summary>Blog Post: NLP, LDA, and Kmeans for Music Classification (in R)</summary>
<p>

[Link](https://www.datacamp.com/community/tutorials/ML-NLP-lyric-analysis)
### LDA Approach
#### Overview of LDA
This paper uses LDA and K-means to generate collections of words from documents that suggest themes.  
The first thing they did was clean the data, remove stop words, create tidy versions.    

Latent Dirichlet Allocation, LDA, is a way to try and discover the latent topic(s) which underlies a specific document. LDA assumes that every document is a combination of one or more of these latent topics. It's like LDA algorithmically goes through words and finds groups of words and makes "clusters" from them which we can give names to as our "topics".   

The idea underpinning this algorithm is that the words which make up one of these topics will appear together in documents. Thus each document gets modeled as a mixture of topics and these topics are themselves defined as a mixture of some words. Then based on the makeup of the words in a document, you can assign a probability that it comes from one of these latent topics.  

#### Algorithm  
1. During initialization, each word is assigned to a random topic  
2. The algorithm goes through each word iteratively and re-assigns the word to a topic with the following considerations:  
* the probability the word belongs to a topic  
* he probability the document will be generated by a topic    
  
#### Applying it to data  
First thing is to create a document-term-matrix (DTM) in which every piece of vocabulary in the corpus are the columns, and every row is a specific document. Thus each value in the DTM is how many times that word is used in that document.    

The key parameters to the algorithm is the k, number of latent topics to assume.    
  
Then the output gives you an associated probability for every single word for every of your k topics. So for the word 'iceberg', you have an assigned score for each of the 3 topics.   

Then you go through your topics and find the words with the highest scores for those topics, and you can see how the algorithm is sort of 'defining' these topics.   

Now we have our words with their topic scores, and so we have our topics 'defined'. Now we can go through our documents, and give the documents scores for each topic! Just as words have a score for every topic, so do documents get a score for every topic.   

We could now look at the topics and which documents fall most heavily into these topics. 

### K-means Approach
#### Overview
K-means isn't going to give a topic score like LDA to each document, it's going to be an all or nothing classification. It is first going to transform each document to a numeric vector and then cluster on 'distance' between them.

### Takeaways  
This is in R so it's not super applicable but it does have good details of LDA and how you could use it and how to think about classifying these documents with LDA and possibly Kmeans. 

</p>
</details>

<details><summary>Blog Post: Word2Vec</summary>
<p>

[Link](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model)

#### The Model
We will be talking about a specific implementation of Word2Vec, called the skip-gram neural network model. This strategy uses a neural network with one hidden layer, and the weights of these hidden layers end up being the 'word vectors' whcih we are trying to learn. This may sound familiar to an auto-encoder. 

#### The Fake Task
We have to set our neural network up to perform a fake task, which later we will come back to and find that by telling our NN to do this, it generated our vectors!
**The Task**: Given a specific word a certain location in a sentence, predict what words will be nearby. This is like a multi-class big classification problem where you want to get probabilities for every other word in your vocabulary to be 'near' this chosen input word. The output probabilities are going to relate to how likely it is find each vocabulary word nearby our input word. For example, if you gave the trained network the input word “Soviet”, the output probabilities are going to be much higher for words like “Union” and “Russia” than for unrelated words like “watermelon” and “kangaroo”.

#### Model Details
So... we're going to need some way to put these words into the neural network. We first build our 'vocabulary' from all of our documents, where we have all of our columns being each of these words, then every word will be represented as a "one-hot" vector, aka a vector where there is a `1` in the column with this word and a `0` elsewhere.

#### The Hidden Layer
Remember the hidden layer is all the different ways to combine your input features, and then those get combined together in order for you to get your output probabilities. For this example, we want each word to represented by a 300 vector representation. Thus we will have 300 nodes in our hidden layer, so that for each word we get an associated weight for each of these 300 nodes. If we think of all of this information as a matrix, where each row is a unique word in our vocabulary, and each column is one of these 300 new feature, each of these rows is now the associated vector for these initial words! We have created vectors from our words, aka 'WordVectors'.

#### TakeAway
If two different words have very similar "contexts", that is they will have similar words which appear around them at high probabilities, then the output from this model for an input of either of these words should be similar. Our network will create the weights to be similar so that these two words have similar word vectors. 

</p>
</details>

<details><summary>Blog Post: Doc2Vec</summary>
<p>

[Link](https://medium.com/scaleabout/a-gentle-introduction-to-doc2vec-db3e8c0cce5e)

### Overview
#### Intro
BOW is one way to represent documents with numbers, but it is pretty crude. LDA is another method. word2vec and doc2vec are alternatives to these, but I will possibly be using them in conjunction with one another? 

#### Word2Vec
word2vec is a concept which is used to map words on to some n-dimensional feature universe. It's a way to make it so that words which appear in similar contexts have a similar vector representation, not just that every word is arbitrarily related to other words. See the above summary of Word2Vec for more details. Ideally, analogies will hold in this word2vec feature space, like man:king should be the same distance as woman:queen are from each other. 

#### Doc2Vec
So the goal of Doc2Vec is create a numeric representation in an n-dimensional feature space, of a document regardless of its length. This is pretty similar to word2vec. Remember for word2vec, the input vector was a one-hot encoding of the word; well now we have a similar encoding where the input is at the document level, and it has a `1` where a word that is included is present, an additional marker for what document this is. This creates document vectors instead of word vectors. 

#### How to use
For training, a set of documents is required. A word vector is generated for each word, and a document vector is generated for each document. In the inference stage, a new document may be presented, and the previously found weights are used to calculate the document vector. 

</p>
</details>

<details><summary>Github Repo, Book Genre From Titles</summary>
<p>

[Link](https://github.com/akshaybhatia10/Book-Genre-Classification)

### README.md
#### Overview
This project classifies book sinto genres based only on titles. There are 32 genres to classify the books in it. There are two notebooks relevant to me: Basic_Bag_of_Words_model.ipynb & Best_TFIDF-Vectorizer_model.ipynb.  

</p>
</details>

<details><summary>Github Repositories, Music Genre Classification</summary>
<p>

[1st Link](https://github.com/dipayandutta93/Music-Genre-Classification-using-lyrics)  
[2nd Link](https://github.com/ianscottknight/Musical-Genre-Classification-of-Song-Lyrics)  

### Link 1
#### Overview

</p>
</details>

<details><summary>Blog Post: Text Classification in Python</summary>
<p>

[Link](https://www.analyticsvidhya.com/blog/2018/04/a-comprehensive-guide-to-understand-and-implement-text-classification-in-python/)

### Setup 
First the article recommends some packages to import: Pandas, sklearn, XGBoost, TextBlob, Keras. The corpus in this case is a couple million labeled amazon reviews as documents.   

### Feature Engineering
They then do some feature engineering, creating input features for our supervised learning model we will build. This is different than taking an unsupervised approach. 

**Count Vectors as a Feature**  
A count vector is creating the DTM discussed in the 'NLP, LDA, and Kmeans for Music Classification' linked above. It has rows as documents and columns as the vocabulary of the corpus.

**TF-IDF Vectors as a Features**  
TF-IDF gives a score to a word. The TF part is the term frequency, so for a word in a document, how often does that word appear in the document? You can normalize by the size of the document, so like what % of the words in the document does this specific word make up?  

Then the IDF part is inverse document frequency, so how few documents actually contain this term. So the TF-IDF is TFxIDF. The point of TF-IDF is to give like a 'uniqueness' score to each word for a specific document, like how much does this word separate this document into some special document category. 

Let's run through a couple examples to get it better. Let's go through 4 scenarios:
* High TF, High IDF = In this document, this term shows up a lot and makes up a high percentage of this document. Also we will multiply this because not many documents have this word in it. Thus this word is important at differentiating this document into its category.
* High TF, Low IDF = In this document this term shows up a lot, but it is also a term which shows up a lot in other documents, so let's lower the weight. 
* Low TF, High IDF = This term doesn't show up that much in this document, but it is a very rare word.
* Low TF, Low IDF = This term doesn't show up very often and it's not a very rare word in the corpus. 

**Word Embeddings**
A word embedding is a form of representing words in a vector space, like in an actual 3D or 4D or n-dimensinoal space. The position of a word within this n-dimensional space is learned from the text, and based on what kinds of words surround it and are normally used with it. You can use pre-trained word embeddings, like word2vec. 

**LDA/Topic Modeling**

### Building the Model
With all these features above, you can build a model with them as the inputs. 

</p>
</details>

<details><summary>Blog Post: NLP basics w/ Python (not only text class.)</summary>
<p>

[Link](https://www.analyticsvidhya.com/blog/2017/01/ultimate-guide-to-understand-implement-natural-language-processing-codes-in-python/)

</p>
</details>

<details><summary>Blog Post: Word Embeddings</summary>
<p>

[Link](https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/)

</p>
</details>  

<details><summary>LDA w/ sklearn</summary>
<p>

[Link](https://medium.com/mlreview/topic-modeling-with-scikit-learn-e80d33668730
)

#### LDA
```python
# get our documents into BOW style.
tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=1000, stop_words='english')
tf = tf_vectorizer.fit_transform(documents)
tf_feature_names = tf_vectorizer.get_feature_names()

# Run LDA
from sklearn.decomposition import LatentDirichletAllocation
lda = LatentDirichletAllocation(n_topics=8, max_iter=5, learning_method='online', learning_offset=50.,random_state=0).fit(tf)

# display
def display_topics(model, feature_names, no_top_words):
    for topic_idx, topic in enumerate(model.components_):
        print "Topic %d:" % (topic_idx)
        print " ".join([feature_names[i]
                        for i in topic.argsort()[:-no_top_words - 1:-1]])

display_topics(lda, tf_feature_names, 10)

```
</p>
</details>  

<details><summary>Custom sklearn Transformers for pipelines</summary>
<p>

[Link](https://towardsdatascience.com/custom-transformers-and-ml-data-pipelines-with-python-20ea2a7adb65)

#### Overview
Remember, Sklearn pipelines are composed of steps, all of which are transforms until the final model fitting. To implement this we take advantage of class inheritance in Python, because we know from Sklearn syntax that we do a lot of class instantiation and stuff in python. The following is a helpful example of normal Sklearn syntax
```python
from sklearn.preprocessing import OneHotEncoder 

#Initializing an object of class OneHotEncoder
one_hot_enc = OneHotEncoder( sparse = True )

#Calling methods on our OneHotEncoder object
one_hot_enc.fit( some_data ) #returns nothing
transformed_data = one_hot_enc.transform( som_data ) #returns something
```
We see above that we create an instance of the OneHotEncoder transformer with some settings, and then we can call the method `.transform` on from that object instance, and give it some_data as an argument. 

Thus when we create our own, we need it to be a class with methods such as `.fit` and `.transform` etc. to fit in with the other transformers. 

#### Getting Started
Sklearn gives us two goo base classes with which to inherit from in order to write out own transformers: `TransformerMixin` & `BaseEstimator`. Inheriting from TransformerMixin ensures that all we need to do is write our fit and transform methods and we get fit_transform for free. Inheriting from BaseEstimator ensures we get get_params and set_params for free. Since the fit method doesn’t need to do anything but return the object itself, all we really need to do after inheriting from these classes, is define the transform method for our custom transformer and we get a fully functional custom transformer that can be  integrated with a scikit-learn pipeline!

#### Example

```python
#Custom Transformer that extracts columns passed as argument to its constructor 
class FeatureSelector( BaseEstimator, TransformerMixin ):
    #Class Constructor 
    def __init__( self, feature_names ):
        self._feature_names = feature_names 
    
    #Return self nothing else to do here    
    def fit( self, X, y = None ):
        return self 
    
    #Method that describes what we need this transformer to do
    def transform( self, X, y = None ):
        return X[ self._feature_names ] 
```
As we can see above, we have created a custom transformer called FeatureSelector, which in order to use we will simply instantiate, and it will already have all of the things from BaseEstimator and TransformerMixin so that's nice. Then we have customized the `.transform()` function to return what we want it to return, based on the arguments which are given when it is called. We could even define other helped functions in this class and then call them in the transform function. Also notice that it must `return` what we want it to return, which is most of the time some minorly altered version of itself.


</p>
</details>  
  
## Things to google search  
* `"genre" classification "book" machine learning text "NLP”`  
* `supervised document classification python`  
* `neural network sklearn multilabel` 

## Misc Notes
<details><summary>Sklearn</summary>
<p>

* **Basic Sklearn syntax**:
```python

[1]: from sklearn.linear_model import Lasso
[2]: from sklearn.model_selection import train_test_split

[3]: X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.4, random_state=42)
[4]: lassoRegObj = Lasso(alpha=0.4)
[5]: lassoRegObj.fit(X_train, y_train)
[6]: lassoRegObj.predict(X_test)
```
The first thing I do here is import my model which I want to use and some more stuff which will be useful, the ability to split my data into a training and testing set. Then I actually perform this split in line 3.

Starting in line 4, I actually begin constructing and fitting my model. Sklearn takes full advantage of python classes and object oriented programming. When you want to fit a model, you first instantiate a model object of that type. You can think of this as retreiving an out-of-the-box fresh model object and then customizing it with your parameters. In line 4 we see this occur, as I save to the `lassoRegObj` an instantiation of a lasso regression object with its alpha value tuned to `0.4`. 

This object has a method, `.fit()` which will customize this model object even more, this time molding it to fit the data which we provide, which is provided as the variables X_train and y_train in this example. 

Now that we have molded this model object to fit our training data, we can have it predict some new data which we provide. 

It is important to realize that you don't have to resave the object every time, because the .fit() is a method which changes the internal state of the model object. 

* **Sklearn CV syntax**: 
```python
[1]: from sklearn.model_selection import GridSearchCV

[2]: param_grid = {'n_neighbors':np.arange(1,50)}
[3]: knn = KNeighborsClassifier()
[4]: knn_cv = GridSearchCV(knn, param_grid, cv=5)
[5]: knn_cv.fit(X,y)
[6]: knn_cv.best_params_
```
OK here we are implementing CV with a model in sklearn. We set up the grid of parameters we would like to search by creating a dictionary called param_grid, where the key is actually the name of one of the hyperparameters which need tuning. 

Then we instantiate our KNN object called `knn`, which is just an out-of-the-box classifier with no customization. But we don't just want to fit this model, we want to fit this model many times each with different hyperparameters. So we need to instantiate a different model with this functionality. That is why on line 4, we instantiate a GridSearchCV object. This object is given our knn model, the hyperparameter ranges to test, and the # of cv folds as its parameters.

Now, as we have many times in the past, we mold this model to fit our data with the `knn_cv.fit()` method. It will fit our chosen model a bunch of times and give us back the best one.  

* **Sklearn Pipeline syntax**: 
```python
[1]: from sklearn.preprocessing import StandardScaler

[2]: X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.4, random_state=42)

[3]: steps = [('scaler', StandardScaler()),
              ('knn', KNeighborsClassifier())]
[4]: pipeline = Pipeline(steps)

[5]: knn_scaled = pipeline.fit(X_train, y_train)
[6]: y_pred = pipeline.predict(X_test)
```
So here we see an example of combining a pre-proccessing step which we call a 'transformation' into a Pipeline. The first thing we do is import some things, then we split up our data set. 

We next construct the steps of our pipeline. In this case, the pipeline has 2 steps, a transformation step called scaler which uses a StandardScaler() object, and then a model fitting step called 'knn' which uses an off-the-shelf KNN. 

Now, we actually instantiate a pipeline object, and this object is the thing which we will now mold and fit to our data. We see, we call `pipeline.fit()` similar to how in the past we have called our model objects, or cv_model objects `.fit()`.

* **Sklearn CV w/Pipeline syntax**: 
```python
[1]: steps = [('scaler', StandardScaler()),
         ('knn', KNeighborsClassifier())]

[2]: pipeline = Pipeline(steps)

[3]: CV_search_parameters = {knn__n_neighbors=np.arange(1, 50)}

[4]: X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=12345)

[5]: cv = GridSearchCV(pipeline, param_grid=CV_search_parameters)

[6]: cv.fit(X_train, y_train)

[7]: y_pred = cv.predict(X_test)
```
Now we are combining a lot of the things we have seen thus far to use CV and also take advantage of pipelines. We begin by creating the steps of our pipeline. This is similar to having a model, but instead of just being a model, it can be a couple transformation steps prior to the model. That is the point of the pipeline, to package it all up into one thing.

As earlier, in line 3, we see us setting which hyperparameters we want to tune through, but because now our Pipeline object can actually contain multiple objects itself, we must identify which pipeline step and what parameter in that step we want to include in out grid search. 

OK so in line 5 we actually instantiate this GridSearchCV object as we did before and we are going to fit it to our data, and instead of just giving it a model object like KNN, we give it an entire pipeline which is needs to run on all the CV folds. 

</p>
</details>  
