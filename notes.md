## Notes for Project  
---
### Articles
<details><summary>NLP, LDA, and Kmeans for Music Classification (in R)</summary>
<p>

[Link](https://www.datacamp.com/community/tutorials/ML-NLP-lyric-analysis)
### LDA Approach
#### Overview of LDA
This paper uses LDA and K-means to generate collections of words from documents that suggest themes.  
The first thing they did was clean the data, remove stop words, create tidy versions.    

Latent Dirichlet Allocation, LDA, is a way to try and discover the latent topic(s) which underlies a specific document. LDA assumes that every document is a combination of one or more of these latent topics. It's like LDA algorithmically goes through words and finds groups of words and makes "clusters" from them which we can give names to as our "topics".   

The idea underpinning this algorithm is that the words which make up one of these topics will appear together in documents. Thus each document gets modeled as a mixture of topics and these topics are themselves defined as a mixture of some words. Then based on the makeup of the words in a document, you can assign a probability that it comes from one of these latent topics.  

#### Algorithm  
1. During initialization, each word is assigned to a random topic  
2. The algorithm goes through each word iteratively and re-assigns the word to a topic with the following considerations:  
* the probability the word belongs to a topic  
* he probability the document will be generated by a topic    
  
#### Applying it to data  
First thing is to create a document-term-matrix (DTM) in which every piece of vocabulary in the corpus are the columns, and every row is a specific document. Thus each value in the DTM is how many times that word is used in that document.    

The key parameters to the algorithm is the k, number of latent topics to assume.    
  
Then the output gives you an associated probability for every single word for every of your k topics. So for the word 'iceberg', you have an assigned score for each of the 3 topics.   

Then you go through your topics and find the words with the highest scores for those topics, and you can see how the algorithm is sort of 'defining' these topics.   

Now we have our words with their topic scores, and so we have our topics 'defined'. Now we can go through our documents, and give the documents scores for each topic! Just as words have a score for every topic, so do documents get a score for every topic.   

We could now look at the topics and which documents fall most heavily into these topics. 

### K-means Approach
#### Overview
K-means isn't going to give a topic score like LDA to each document, it's going to be an all or nothing classification. It is first going to transform each document to a numeric vector and then cluster on 'distance' between them.

### Takeaways  
This is in R so it's not super applicable but it does have good details of LDA and how you could use it and how to think about classifying these documents with LDA and possibly Kmeans.

</p>
</details>

<details><summary>Doc2Vec</summary>
<p>

[Link](https://medium.com/scaleabout/a-gentle-introduction-to-doc2vec-db3e8c0cce5e)

</p>
</details>

<details><summary>Title</summary>
<p>

[Link](https://medium.com/@hakanakyurek)

* hakanaska@gmail.com

</p>
</details>

</details>

<details><summary>Title2</summary>
<p>

[Link](https://github.com/akshaybhatia10/Book-Genre-Classification)

</p>
</details>

<details><summary>Text Classification in Python</summary>
<p>

[Link](https://www.analyticsvidhya.com/blog/2018/04/a-comprehensive-guide-to-understand-and-implement-text-classification-in-python/)

### Setup 
First the article recommends some packages to import: Pandas, sklearn, XGBoost, TextBlob, Keras. The corpus in this case is a couple million labeled amazon reviews as documents.   

### Feature Engineering
They then do some feature engineering, creating input features for our supervised learning model we will build. This is different than taking an unsupervised approach. 

**Count Vectors as a Feature**  
A count vector is creating the DTM discussed in the 'NLP, LDA, and Kmeans for Music Classification' linked above. It has rows as documents and columns as the vocabulary of the corpus.

**TF-IDF Vectors as a Features**  
TF-IDF gives a score to a word. The TF part is the term frequency, so for a word in a document, how often does that word appear in the document? You can normalize by the size of the document, so like what % of the words in the document does this specific word make up?  

Then the IDF part is inverse document frequency, so how few documents actually contain this term. So the TF-IDF is TFxIDF. The point of TF-IDF is to give like a 'uniqueness' score to each word for a specific document, like how much does this word separate this document into some special document category. 

Let's run through a couple examples to get it better. Let's go through 4 scenarios:
* High TF, High IDF = In this document, this term shows up a lot and makes up a high percentage of this document. Also we will multiply this because not many documents have this word in it. Thus this word is important at differentiating this document into its category.
* High TF, Low IDF = In this document this term shows up a lot, but it is also a term which shows up a lot in other documents, so let's lower the weight. 
* Low TF, High IDF = This term doesn't show up that much in this document, but it is a very rare word.
* Low TF, Low IDF = This term doesn't show up very often and it's not a very rare word in the corpus. 

**Word Embeddings**
A word embedding is a form of representing words in a vector space, like in an actual 3D or 4D or n-dimensinoal space. The position of a word within this n-dimensional space is learned from the text, and based on what kinds of words surround it and are normally used with it. You can use pre-trained word embeddings, like word2vec. 

**LDA/Topic Modeling**

### Building the Model
With all these features above, you can build a model with them as the inputs. 


</p>
</details>

<details><summary>NLP basics w/ Python (not only text class.)</summary>
<p>

[Link](https://www.analyticsvidhya.com/blog/2017/01/ultimate-guide-to-understand-implement-natural-language-processing-codes-in-python/)

</p>
</details>

<details><summary>Title6</summary>
<p>

[Link](https://www.kaggle.com/c/wise-2014/overview)

</p>
</details>

<details><summary>Title7</summary>
<p>

[Link](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html#sklearn.neural_network.MLPClassifier)

</p>
</details>  

<details><summary>Title8</summary>
<p>

[Link](http://ethen8181.github.io/machine-learning/clustering_old/topic_model/LDA.html#content)

</p>
</details>  

<details><summary>Word Embeddings</summary>
<p>

[Link](https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/)

</p>
</details>  

  
### Things to google search  
* `"genre" classification "book" machine learning text "NLP‚Äù`  
* `supervised document classification python`  
* `neural network sklearn multilabel` 
